# generasi-berita-encoder-decoder-transformer

## ğŸ“ˆ Hasil

#### Basic LSTM dan Attention LSTM gagal menghasilkan ringkasan yang bermakna.
#### Transformer menghasilkan output yang sedikit lebih baik, walaupun BLEU Score masih rendah.
#### Performa keseluruhan dibatasi oleh ukuran dataset yang kecil dan desain model yang masih sederhana.

#   

## ğŸ“‹ Catatan & Rencana Pengembangan

#### Perbesar ukuran dataset untuk hasil yang lebih akurat.
#### Fine-tuning hyperparameter seperti learning rate, batch size, dan jumlah layer.
#### Gunakan pre-trained embeddings seperti GloVe untuk memperkaya representasi kata.
#### Coba varian Transformer yang lebih canggih seperti BERT2BERT atau T5.

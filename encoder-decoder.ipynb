{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379bfb84-a195-4155-92d6-90d6e7b4c1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.2 in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchtext==0.17.2 in /opt/conda/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: scikit-learn==1.4.2 in /opt/conda/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: tqdm==4.66.2 in /opt/conda/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: nltk==3.8.1 in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas==2.2.2) (2024.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.17.2) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.4.2) (3.5.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.6.77)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.2) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.2) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Basic LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0867d19c504b95944835df9aad5d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:13<00:00,  1.04s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  In COLING 2004.\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  In COLING 2004.\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  In COLING 2004.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Attention LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:11<00:00,  5.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:08<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic LSTM\n",
      "Training Time: 13.51 sec\n",
      "Average BLEU Score: 0.0000\n",
      "\n",
      "Predicted: the the of\n",
      "Ground Truth: (image credit: android central) this is android central's news weekly, your go-to source for a concise roundup of the week's most significant tech stories\n",
      "\n",
      "Predicted: the the of the\n",
      "Ground Truth: mechanical keycaps are one of the key components of mechanical keyboards, as they correspond to the alphanumeric characters they represent\n",
      "\n",
      "Predicted: the the of\n",
      "Ground Truth: editor’s note keeping dark circles and puffiness around the eye at bay is an easy way to keep your overall face looking younger and more vibrant\n",
      "\n",
      "Predicted: the the of\n",
      "Ground Truth: after stepping through a floo chamber and teleporting in a puff of green smoke to a vast and stunning re-creation of the ministry of magic, i found myself boarding an elevator\n",
      "\n",
      "Predicted: the the of\n",
      "Ground Truth: game pass subscribers can dive into a whimsical and magical world inspired by southern folklore now in south of midnight\n",
      "\n",
      "Attention LSTM\n",
      "Training Time: 71.29 sec\n",
      "Average BLEU Score: 0.0000\n",
      "\n",
      "Predicted: the the of of of of of of of of of of of of of of of of of of of of of of of of of of of of of to to to to to to\n",
      "Ground Truth: (image credit: android central) this is android central's news weekly, your go-to source for a concise roundup of the week's most significant tech stories\n",
      "\n",
      "Predicted: the the of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of to of to\n",
      "Ground Truth: mechanical keycaps are one of the key components of mechanical keyboards, as they correspond to the alphanumeric characters they represent\n",
      "\n",
      "Predicted: the the of of of of of of of of of of of of of of to of of of of of of of of to to to to to to to to to to to to\n",
      "Ground Truth: editor’s note keeping dark circles and puffiness around the eye at bay is an easy way to keep your overall face looking younger and more vibrant\n",
      "\n",
      "Predicted: the the of of of of of of of of of of of of of of of to of of of of of of of of to to to to to to to to to to to\n",
      "Ground Truth: after stepping through a floo chamber and teleporting in a puff of green smoke to a vast and stunning re-creation of the ministry of magic, i found myself boarding an elevator\n",
      "\n",
      "Predicted: the the of of of of of of of of of of of of of of of of of of of of of of of of of of to to to to to to to to to\n",
      "Ground Truth: game pass subscribers can dive into a whimsical and magical world inspired by southern folklore now in south of midnight\n",
      "\n",
      "Transformer\n",
      "Training Time: 8.57 sec\n",
      "Average BLEU Score: 0.0010\n",
      "\n",
      "Predicted: to to the a in this is a and a in of in to for a to a of the in to to a in and and and and and and and and and and and and\n",
      "Ground Truth: (image credit: android central) this is android central's news weekly, your go-to source for a concise roundup of the week's most significant tech stories\n",
      "\n",
      "Predicted: to in a are a of the the in of in and as the of to the to to the in and and and and and and and and and and and and and and and and\n",
      "Ground Truth: mechanical keycaps are one of the key components of mechanical keyboards, as they correspond to the alphanumeric characters they represent\n",
      "\n",
      "Predicted: to a a a to to and a of the in to to is an in a to to to to of in a and in a and and and and and and and and and and\n",
      "Ground Truth: editor’s note keeping dark circles and puffiness around the eye at bay is an easy way to keep your overall face looking younger and more vibrant\n",
      "\n",
      "Predicted: to and in in a a in and a in a in of in and to a a and of in of the a of in of and a in an the and and and and and\n",
      "Ground Truth: after stepping through a floo chamber and teleporting in a puff of green smoke to a vast and stunning re-creation of the ministry of magic, i found myself boarding an elevator\n",
      "\n",
      "Predicted: to of a the to in in a a and in in of to in a and in to of to and and and and and and and and and and and and and and and and\n",
      "Ground Truth: game pass subscribers can dive into a whimsical and magical world inspired by southern folklore now in south of midnight\n"
     ]
    }
   ],
   "source": [
    "# Install Library\n",
    "%pip install torch==2.2.2 pandas==2.2.2 torchtext==0.17.2 scikit-learn==1.4.2 tqdm==4.66.2 nltk==3.8.1\n",
    "\n",
    "# Import Library\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import time\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('scraped_news_technology.csv')\n",
    "df = df.dropna(subset=['content'])\n",
    "df = df[df['content'].str.strip() != '']\n",
    "df = df.head(1000)\n",
    "df['title'] = df['content'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# Build Vocab\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield text.lower().split()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['content'].tolist() + df['title'].tolist()), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab.set_default_index(vocab['<pad>'])\n",
    "pad_idx = vocab['<pad>']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Dataset & Loader\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, contents, titles):\n",
    "        self.contents = contents\n",
    "        self.titles = titles\n",
    "    def __len__(self):\n",
    "        return len(self.contents)\n",
    "    def __getitem__(self, idx):\n",
    "        content = self.contents[idx].lower().split()\n",
    "        title = self.titles[idx].lower().split()\n",
    "        content_idx = torch.tensor(vocab(content), dtype=torch.long)\n",
    "        title_idx = torch.tensor([vocab['<sos>']] + vocab(title) + [vocab['<eos>']], dtype=torch.long)\n",
    "        return content_idx, title_idx\n",
    "\n",
    "def collate_fn(batch):\n",
    "    contents, titles = zip(*batch)\n",
    "    contents_pad = pad_sequence(contents, batch_first=True, padding_value=pad_idx)\n",
    "    titles_pad = pad_sequence(titles, batch_first=True, padding_value=pad_idx)\n",
    "    return contents_pad, titles_pad\n",
    "\n",
    "contents = df['content'].tolist()\n",
    "titles = df['title'].tolist()\n",
    "train_contents, val_contents, train_titles, val_titles = train_test_split(contents, titles, test_size=0.2)\n",
    "train_dataset = NewsDataset(train_contents, train_titles)\n",
    "val_dataset = NewsDataset(val_contents, val_titles)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ================ Define Model ================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(DEVICE)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:,0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:,t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:,t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        return outputs\n",
    "\n",
    "# Attention Model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0),1).unsqueeze(1)\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class AttentionSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTMCell(emb_dim + hid_dim, hid_dim)\n",
    "        self.attention = Attention(hid_dim)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        embedded_src = self.embedding(src)\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(DEVICE)\n",
    "\n",
    "        input = trg[:,0]\n",
    "        decoder_hidden, decoder_cell = hidden[-1], cell[-1]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            embedded_input = self.embedding(input)\n",
    "            attn_weights = self.attention(decoder_hidden.unsqueeze(0), encoder_outputs)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            decoder_input = torch.cat((embedded_input, context), dim=1)\n",
    "            decoder_hidden, decoder_cell = self.decoder(decoder_input, (decoder_hidden, decoder_cell))\n",
    "            output = self.fc_out(decoder_hidden)\n",
    "            outputs[:,t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:,t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.transformer = nn.Transformer(emb_dim, nhead, num_layers, num_layers)\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_emb = self.embedding(src).permute(1,0,2)\n",
    "        trg_emb = self.embedding(trg).permute(1,0,2)\n",
    "        outputs = self.transformer(src_emb, trg_emb)\n",
    "        output = self.fc_out(outputs)\n",
    "        return output.permute(1,0,2)\n",
    "\n",
    "# ================ Training Function ================\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in tqdm(loader):\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:,1:].reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "# ================ Evaluation ================\n",
    "def generate(model, loader, max_len=20):\n",
    "    model.eval()\n",
    "    examples = []\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0.0) if isinstance(model, (Seq2Seq, AttentionSeq2Seq)) else model(src, trg)\n",
    "            output = outputs.argmax(-1)\n",
    "            for idx in range(src.size(0)):\n",
    "                pred_tokens = [vocab.get_itos()[i] for i in output[idx].tolist() if i not in [pad_idx, vocab['<sos>'], vocab['<eos>']]]\n",
    "                true_tokens = [vocab.get_itos()[i] for i in trg[idx].tolist() if i not in [pad_idx, vocab['<sos>'], vocab['<eos>']]]\n",
    "                examples.append((\" \".join(pred_tokens), \" \".join(true_tokens)))\n",
    "    return examples\n",
    "\n",
    "# ================ Run All ================\n",
    "models = {\n",
    "    'Basic LSTM': Seq2Seq(Encoder(vocab_size, 256, 512), Decoder(vocab_size, 256, 512)),\n",
    "    'Attention LSTM': AttentionSeq2Seq(vocab_size, 256, 512),\n",
    "    'Transformer': TransformerModel(vocab_size, 256, 8, 3)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    start = time.time()\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    end = time.time()\n",
    "    samples = generate(model, val_loader)\n",
    "    bleu_scores = [sentence_bleu([true.split()], pred.split()) for pred, true in samples]\n",
    "    results[name] = {'time': end-start, 'bleu': sum(bleu_scores)/len(bleu_scores), 'samples': samples[:5]}\n",
    "\n",
    "# Show Results\n",
    "for name, res in results.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"Training Time: {res['time']:.2f} sec\")\n",
    "    print(f\"Average BLEU Score: {res['bleu']:.4f}\")\n",
    "    for pred, true in res['samples']:\n",
    "        print(f\"\\nPredicted: {pred}\\nGround Truth: {true}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
